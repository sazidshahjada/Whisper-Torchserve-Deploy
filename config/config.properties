# Basic TorchServe configuration
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082

# Use GPU or CPU for inference
# Valid values: "gpu" or "cpu"
# Default value: "gpu"
inference_device=gpu

# Increase these values to handle large files
max_request_size=150000000
max_response_size=150000000
default_response_timeout=900

# Additional performance settings (optional)
number_of_netty_threads=32
job_queue_size=1000